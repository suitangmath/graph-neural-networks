{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compact Tutorial\n",
    "\n",
    "Note:\n",
    "1. This is a codeblock-only version of the tutorial.\n",
    "2. Some code has been updated to fix the wrong module\n",
    "  - `Utils` should be `alegnn.utils`\n",
    "  - `Modules` should be `alegnn.modules`\n",
    "3. The `matplotlib` part in the end has been skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['text.usetex'] = True # Comment this line if no LaTeX installation is available\n",
    "matplotlib.rcParams['font.family'] = 'serif' # Comment this line if no LaTeX installation is available\n",
    "matplotlib.rcParams['text.latex.preamble'] = r'\\usepackage{amsmath}'\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; torch.set_default_dtype(torch.float64)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alegnn.utils.graphTools as graphTools\n",
    "import alegnn.utils.dataTools\n",
    "import alegnn.utils.graphML as gml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alegnn.modules.architectures as archit\n",
    "import alegnn.modules.model as model\n",
    "import alegnn.modules.training as training\n",
    "import alegnn.modules.evaluation as evaluation\n",
    "import alegnn.modules.loss as loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alegnn.utils.miscTools import writeVarValues\n",
    "from alegnn.utils.miscTools import saveSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphType = 'SBM' # Type of graph\n",
    "thisFilename = 'sourceLocTutorial' # This is the general name of all related files\n",
    "saveDirRoot = 'experiments' # Relative location where to save the file\n",
    "saveDir = os.path.join(saveDirRoot, thisFilename) # Dir where to save all the results from each run\n",
    "\n",
    "#\\\\\\ Create .txt to store the values of the setting parameters for easier\n",
    "# reference when running multiple experiments\n",
    "today = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "# Append date and time of the run to the directory, to avoid several runs of\n",
    "# overwritting each other.\n",
    "saveDir = saveDir + '-' + graphType + '-' + today\n",
    "# Create directory\n",
    "if not os.path.exists(saveDir):\n",
    "    os.makedirs(saveDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the file where all the (hyper)parameters are results will be saved.\n",
    "varsFile = os.path.join(saveDir,'hyperparameters.txt')\n",
    "with open(varsFile, 'w+') as file:\n",
    "    file.write('%s\\n\\n' % datetime.datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "useGPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\\\\\ Save seeds for reproducibility\n",
    "#   PyTorch seeds\n",
    "torchState = torch.get_rng_state()\n",
    "torchSeed = torch.initial_seed()\n",
    "#   Numpy seeds\n",
    "numpyState = np.random.RandomState().get_state()\n",
    "#   Collect all random states\n",
    "randomStates = []\n",
    "randomStates.append({})\n",
    "randomStates[0]['module'] = 'numpy'\n",
    "randomStates[0]['state'] = numpyState\n",
    "randomStates.append({})\n",
    "randomStates[1]['module'] = 'torch'\n",
    "randomStates[1]['state'] = torchState\n",
    "randomStates[1]['seed'] = torchSeed\n",
    "#   This list and dictionary follows the format to then be loaded, if needed,\n",
    "#   by calling the loadSeed function in Utils.miscTools\n",
    "saveSeed(randomStates, saveDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrain = 5000 # Number of training samples\n",
    "nValid = int(0.2 * nTrain) # Number of validation samples\n",
    "nTest = 50 # Number of testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nNodes = 20 # Number of nodes\n",
    "nClasses = 2 # Number of classes (i.e. number of communities)\n",
    "graphOptions = {} # Dictionary of options to pass to the graphTools.createGraph function\n",
    "graphOptions['nCommunities'] = nClasses # Number of communities\n",
    "graphOptions['probIntra'] = 0.8 # Probability of drawing edges intra communities\n",
    "graphOptions['probInter'] = 0.2 # Probability of drawing edges inter communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tMax = None # Maximum number of diffusion times (W^t for t < tMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\\\\\ Save values:\n",
    "writeVarValues(varsFile, {'nNodes': nNodes, 'graphType': graphType})\n",
    "writeVarValues(varsFile, graphOptions)\n",
    "writeVarValues(varsFile, {'nTrain': nTest,\n",
    "                          'nValid': nValid,\n",
    "                          'nTest': nTest,\n",
    "                          'tMax': tMax,\n",
    "                          'nClasses': nClasses,\n",
    "                          'useGPU': useGPU})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFunction = nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = training.Trainer\n",
    "evaluator = evaluation.evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimAlg = 'ADAM'\n",
    "learningRate = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpochs = 4 # Number of epochs\n",
    "batchSize = 20 # Batch size\n",
    "validationInterval = 20 # How many training steps to do the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeVarValues(varsFile,\n",
    "               {'optimAlg': optimAlg,\n",
    "                'learningRate': learningRate,\n",
    "                'beta1': beta1,\n",
    "                'lossFunction': lossFunction,\n",
    "                'nEpochs': nEpochs,\n",
    "                'batchSize': batchSize,\n",
    "                'validationInterval': validationInterval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hParamsAggGNN = {}\n",
    "hParamsAggGNN['name'] = 'AggGNN' # We give a name to this architecture\n",
    "\n",
    "hParamsAggGNN['nNodes'] = 1 # The nodes are selected starting from the \n",
    "    # top of the signal vector, for the order given in the data. Later\n",
    "    # we reorder the data to follow the highest-degree criteria.\n",
    "hParamsAggGNN['Nmax'] = None # If 'None' sets maxN equal to the size\n",
    "    # of the graph, so that no information is lost when creating the\n",
    "    # aggregation sequence z_{i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hParamsAggGNN['order'] = 'Degree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hParamsAggGNN['F'] = [1, 5, 5] # Features per layer (the first element is the number of input features)\n",
    "hParamsAggGNN['K'] = [3, 3] # Number of filter taps per layer\n",
    "hParamsAggGNN['bias'] = True # Decide whether to include a bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hParamsAggGNN['sigma'] = nn.ReLU # Selected nonlinearity\n",
    "hParamsAggGNN['rho'] = nn.MaxPool1d # Pooling function\n",
    "hParamsAggGNN['alpha'] = [2, 3] # Size of pooling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hParamsAggGNN['dimLayersMLP'] = [nClasses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeVarValues(varsFile, hParamsAggGNN)\n",
    "modelList += [hParamsAggGNN['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hParamsSelGNN = {} # Create the dictionary to save the hyperparameters\n",
    "hParamsSelGNN['name'] = 'SelGNN' # Name the architecture\n",
    "\n",
    "hParamsSelGNN['F'] = [1, 5, 5] # Features per layer (first element is the number of input features)\n",
    "hParamsSelGNN['K'] = [3, 3] # Number of filter taps per layer\n",
    "hParamsSelGNN['bias'] = True # Decide whether to include a bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hParamsSelGNN['sigma'] = nn.ReLU # Selected nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hParamsSelGNN['rho'] = gml.MaxPoolLocal # Summarizing function\n",
    "hParamsSelGNN['alpha'] = [2, 3] # alpha-hop neighborhood that\n",
    "hParamsSelGNN['N'] = [10, 5] # Number of nodes to keep at the end of each layer is affected by the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hParamsSelGNN['order'] = 'Degree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hParamsSelGNN['dimLayersMLP'] = [nClasses] # Dimension of the fully connected layers after the GCN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeVarValues(varsFile, hParamsSelGNN)\n",
    "modelList += [hParamsSelGNN['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hParamsCrsGNN = deepcopy(hParamsSelGNN)\n",
    "hParamsCrsGNN['name'] = 'CrsGNN'\n",
    "hParamsCrsGNN['rho'] = nn.MaxPool1d\n",
    "hParamsCrsGNN['order'] = None # We don't need any special ordering, since\n",
    "    # it will be determined by the hierarchical clustering algorithm\n",
    "\n",
    "writeVarValues(varsFile, hParamsCrsGNN)\n",
    "modelList += [hParamsCrsGNN['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "printInterval = 0 # After how many training steps, print the partial results\n",
    "    # if 0 never print training partial results.\n",
    "xAxisMultiplierTrain = 100 # How many training steps in between those shown in\n",
    "    # the plot, i.e., one training step every xAxisMultiplierTrain is shown.\n",
    "xAxisMultiplierValid = 10 # How many validation steps in between those shown,\n",
    "    # same as above.\n",
    "figSize = 5 # Overall size of the figure that contains the plot\n",
    "lineWidth = 2 # Width of the plot lines\n",
    "markerShape = 'o' # Shape of the markers\n",
    "markerSize = 3 # Size of the markers\n",
    "\n",
    "writeVarValues(varsFile,\n",
    "               {'saveDir': saveDir,\n",
    "                'printInterval': printInterval,\n",
    "                'figSize': figSize,\n",
    "                'lineWidth': lineWidth,\n",
    "                'markerShape': markerShape,\n",
    "                'markerSize': markerSize})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device selected: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if useGPU and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = 'cpu'\n",
    "# Notify:\n",
    "print(\"Device selected: %s\" % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingOptions = {}\n",
    "\n",
    "trainingOptions['saveDir'] = saveDir\n",
    "trainingOptions['printInterval'] = printInterval\n",
    "trainingOptions['validationInterval'] = validationInterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = graphTools.Graph(graphType, nNodes, graphOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.computeGFT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceNodes = graphTools.computeSourceNodes(G.A, nClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeVarValues(varsFile, {'sourceNodes': sourceNodes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = alegnn.utils.dataTools.SourceLocalization(G, nTrain, nValid, nTest, sourceNodes, tMax = tMax)\n",
    "data.astype(torch.float64)\n",
    "data.expandDims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsGNN = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisName = hParamsAggGNN['name']\n",
    "\n",
    "#\\\\\\ Architecture\n",
    "thisArchit = archit.AggregationGNN(# Linear\n",
    "                                   hParamsAggGNN['F'],\n",
    "                                   hParamsAggGNN['K'],\n",
    "                                   hParamsAggGNN['bias'],\n",
    "                                   # Nonlinearity\n",
    "                                   hParamsAggGNN['sigma'],\n",
    "                                   # Pooling\n",
    "                                   hParamsAggGNN['rho'],\n",
    "                                   hParamsAggGNN['alpha'],\n",
    "                                   # MLP in the end\n",
    "                                   hParamsAggGNN['dimLayersMLP'],\n",
    "                                   # Structure\n",
    "                                   G.S/np.max(np.diag(G.E)), # Normalize the adjacency matrix\n",
    "                                   order = hParamsAggGNN['order'],\n",
    "                                   maxN = hParamsAggGNN['Nmax'],\n",
    "                                   nNodes = hParamsAggGNN['nNodes'])\n",
    "\n",
    "#\\\\\\ Optimizer\n",
    "thisOptim = optim.Adam(thisArchit.parameters(), lr = learningRate, betas = (beta1,beta2))\n",
    "\n",
    "#\\\\\\ Model\n",
    "AggGNN = model.Model(thisArchit,\n",
    "                     lossFunction(),\n",
    "                     thisOptim,\n",
    "                     trainer,\n",
    "                     evaluator,\n",
    "                     device,\n",
    "                     thisName,\n",
    "                     saveDir)\n",
    "\n",
    "#\\\\\\ Add model to the dictionary\n",
    "modelsGNN[thisName] = AggGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisName = hParamsSelGNN['name']\n",
    "\n",
    "#\\\\\\ Architecture\n",
    "thisArchit = archit.SelectionGNN(# Graph filtering\n",
    "                                 hParamsSelGNN['F'],\n",
    "                                 hParamsSelGNN['K'],\n",
    "                                 hParamsSelGNN['bias'],\n",
    "                                 # Nonlinearity\n",
    "                                 hParamsSelGNN['sigma'],\n",
    "                                 # Pooling\n",
    "                                 hParamsSelGNN['N'],\n",
    "                                 hParamsSelGNN['rho'],\n",
    "                                 hParamsSelGNN['alpha'],\n",
    "                                 # MLP\n",
    "                                 hParamsSelGNN['dimLayersMLP'],\n",
    "                                 # Structure\n",
    "                                 G.S/np.max(np.real(G.E)), # Normalize adjacency\n",
    "                                 order = hParamsSelGNN['order'])\n",
    "# This is necessary to move all the learnable parameters to be\n",
    "# stored in the device (mostly, if it's a GPU)\n",
    "thisArchit.to(device)\n",
    "\n",
    "#\\\\\\ Optimizer\n",
    "thisOptim = optim.Adam(thisArchit.parameters(), lr = learningRate, betas = (beta1,beta2))\n",
    "\n",
    "#\\\\\\ Model\n",
    "SelGNN = model.Model(thisArchit,\n",
    "                     lossFunction(),\n",
    "                     thisOptim,\n",
    "                     trainer,\n",
    "                     evaluator,\n",
    "                     device,\n",
    "                     thisName,\n",
    "                     saveDir)\n",
    "\n",
    "#\\\\\\ Add model to the dictionary\n",
    "modelsGNN[thisName] = SelGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisName = hParamsCrsGNN['name']\n",
    "\n",
    "#\\\\\\ Architecture\n",
    "thisArchit = archit.SelectionGNN(# Graph filtering\n",
    "                                 hParamsCrsGNN['F'],\n",
    "                                 hParamsCrsGNN['K'],\n",
    "                                 hParamsCrsGNN['bias'],\n",
    "                                 # Nonlinearity\n",
    "                                 hParamsCrsGNN['sigma'],\n",
    "                                 # Pooling\n",
    "                                 hParamsCrsGNN['N'],\n",
    "                                 hParamsCrsGNN['rho'],\n",
    "                                 hParamsCrsGNN['alpha'],\n",
    "                                 # MLP\n",
    "                                 hParamsCrsGNN['dimLayersMLP'],\n",
    "                                 # Structure\n",
    "                                 G.S/np.max(np.real(G.E)),\n",
    "                                 coarsening = True)\n",
    "# This is necessary to move all the learnable parameters to be\n",
    "# stored in the device (mostly, if it's a GPU)\n",
    "thisArchit.to(device)\n",
    "\n",
    "#\\\\\\ Optimizer\n",
    "thisOptim = optim.Adam(thisArchit.parameters(), lr = learningRate, betas = (beta1,beta2))\n",
    "\n",
    "#\\\\\\ Model\n",
    "CrsGNN = model.Model(thisArchit,\n",
    "                     lossFunction(),\n",
    "                     thisOptim,\n",
    "                     trainer,\n",
    "                     evaluator,\n",
    "                     device,\n",
    "                     thisName,\n",
    "                     saveDir)\n",
    "\n",
    "#\\\\\\ Add model to the dictionary\n",
    "modelsGNN[thisName] = CrsGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossTrain = {}\n",
    "costTrain = {}\n",
    "lossValid = {}\n",
    "costValid = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model AggGNN... OK\n",
      "Training model SelGNN... OK\n",
      "Training model CrsGNN... OK\n"
     ]
    }
   ],
   "source": [
    "for thisModel in modelsGNN.keys():\n",
    "    print(\"Training model %s...\" % thisModel, end = ' ', flush = True)\n",
    "    \n",
    "    #Train\n",
    "    thisTrainVars = modelsGNN[thisModel].train(data, nEpochs, batchSize, **trainingOptions)\n",
    "    # Save the variables\n",
    "    lossTrain[thisModel] = thisTrainVars['lossTrain']\n",
    "    costTrain[thisModel] = thisTrainVars['costTrain']\n",
    "    lossValid[thisModel] = thisTrainVars['lossValid']\n",
    "    costValid[thisModel] = thisTrainVars['costValid']\n",
    "    \n",
    "    print(\"OK\", flush = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "costBest = {} # Classification accuracy obtained for the best model\n",
    "costLast = {} # Classification accuracy obtained for the last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thisModel in modelsGNN.keys():\n",
    "    thisEvalVars = modelsGNN[thisModel].evaluate(data)\n",
    "    \n",
    "    costBest[thisModel] = thisEvalVars['costBest']\n",
    "    costLast[thisModel] = thisEvalVars['costLast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final evaluations\n",
      "\tAggGNN:  48.00% [Best]  52.00% [Last]\n",
      "\tSelGNN:  22.00% [Best]  52.00% [Last]\n",
      "\tCrsGNN:  40.00% [Best]  48.00% [Last]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal evaluations\")\n",
    "for thisModel in modelList:\n",
    "    print(\"\\t%s: %6.2f%% [Best] %6.2f%% [Last]\" % (\n",
    "            thisModel,\n",
    "            costBest[thisModel] * 100,\n",
    "            costLast[thisModel] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
